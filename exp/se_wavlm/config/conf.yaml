input_size: &input_size 1024 # WavLM output Dimension
kmeans_ckpt: &kmeans_ckpt /public/home/qinxy/bltang/kmeans/LibriSpeech_wavlm_k1000_L7.pt



## Optim
optim: 
    type: torch.optim.Adam
    args:
        lr: 1.0e-3
scheduler: 
    type: funcodec.schedulers.warmup_lr.WarmupLR
    args:
        warmup_steps: 10000
## Model
model:
    type: exp.laura_gpt_se.model.LauraGenModel
    args:
        codec_sampling_ratio: 0.5
        lsm_weight: 0.0
        length_normalized_loss: true
        predict_nq: 1
        codec_conf:
            codebook_size: 1000 # Output Vocabulary size - <EOS>
            codebook_dim: *input_size # Embedding Dimension
        codec_lm_conf:
            name: transformer
            pos_enc: rel_pos
            selfattention_layer_type: rel_selfattn
            embed_unit: *input_size # input dimension to the decoder LM
            att_unit: *input_size
            head: 8
            unit: 2048
            layer: 6
            dropout_rate: 0.1
            pe_type: uni
            bidirectional_inputs: true
            codec_groups: 1
