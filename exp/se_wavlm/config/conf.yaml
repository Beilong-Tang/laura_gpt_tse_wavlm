train_shape_file: ["/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/Libri2Mix/wav16k/min/wavlm_6/train/clean/shape.scp"]
valid_shape_file: ["/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/Libri2Mix/wav16k/min/wavlm_6/test/clean/shape.scp"]
train_data_path_and_name_and_type: [
    [
        "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/Libri2Mix/wav16k/min/wavlm_6/train/mix/all.scp",
        "text",
        "npy"
    ],
    [
        "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/Libri2Mix/wav16k/min/wavlm_6/train/clean/all.scp",
        "codec",
        "npy"
    ]
]
valid_data_path_and_name_and_type: [
     [
        "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/Libri2Mix/wav16k/min/wavlm_6/test/mix/all.scp",
        "text",
        "npy"
    ],
    [
        "/public/home/qinxy/bltang/data/LibriMix/Libri2Mix/Libri2Mix/wav16k/min/wavlm_6/test/clean/all.scp",
        "codec",
        "npy"
    ]
]


input_size: &input_size 1000
input_dim: &input_dim 1024 # Wavlm Input Dimension
kmeans_ckpt: &kmeans_ckpt /public/home/qinxy/bltang/kmeans/LibriSpeech_wavlm_k1000_L7.pt
use_preprocessor: false
audio_max_duration: 60
codec_token_rate: 25
seed: 1234


## Model
model:
    type: exp.laura_gpt_se.model.LauraGenModel
    args:
        kmeans_ckpt: *kmeans_ckpt
        codec_sampling_ratio: 0.5
        lsm_weight: 0.0
        length_normalized_loss: true
        predict_nq: 1
        codec_conf:
            codebook_size: *input_size # Output Vocabulary size - <EOS>
            codebook_dim: *input_dim # Embedding Dimension
        codec_lm_conf:
            name: transformer
            pos_enc: rel_pos
            selfattention_layer_type: rel_selfattn
            embed_unit: *input_dim # input dimension to the decoder LM
            att_unit: *input_dim
            head: 8
            unit: 2048
            layer: 6
            dropout_rate: 0.1
            pe_type: uni
            bidirectional_inputs: true
            codec_groups: 1

## Optim
optim: 
    type: torch.optim.Adam
    args:
        lr: 1.0e-3
scheduler: 
    type: funcodec.schedulers.warmup_lr.WarmupLR
    args:
        warmup_steps: 10000

### Training related
batch_type: length
batch_bins: 40960
batch_size: 80 # This does not matter here
sort_in_batch: descending
sort_batch: descending
num_workers: 8
max_cache_size: 0.0
max_cache_fd: 32
train_dtype: float32


best_field: loss
best_save_type: descend
max_ckpt: 1
log_interval: 10
epoch: 50

## Add for argument type checking, Does not matter here
allow_variable_data_keys: false
drop_last: false
fold_length: []
